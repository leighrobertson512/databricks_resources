# Databricks notebook source
# Generated by LakehousePlumber
# Pipeline: silver_weather_transform
# FlowGroup: weather_data_cleansing

import dlt

# Pipeline Configuration
PIPELINE_ID = "silver_weather_transform"
FLOWGROUP_ID = "weather_data_cleansing"

# ============================================================================
# SOURCE VIEWS
# ============================================================================

@dlt.view()
def v_bronze_forecasts_preprocessed():
    """Load and preprocess bronze weather forecast data"""
    df = spark.read.table("{{ catalog }}.{{ bronze_schema }}.{{ bronze_forecasts_table }}")

    return df


# ============================================================================
# TRANSFORMATION VIEWS
# ============================================================================

@dlt.view(comment="Clean and transform weather data with timezone conversion")
def v_weather_data_transformed():
    """Clean and transform weather data with timezone conversion"""
    return spark.sql("""SELECT
  post_code,
  number,
  name,
  -- Extract timezone and convert timestamps
  regexp_extract(startTime, '([+-]\\d{2}:\\d{2})$', 1) as timezone_offset,
  regexp_replace(startTime, '[+-]\\d{2}:\\d{2}$', '') as start_time_clean,
  regexp_replace(endTime, '[+-]\\d{2}:\\d{2}$', '') as end_time_clean,

  -- Convert to UTC timestamps
  CASE
    WHEN regexp_extract(startTime, '([+-]\\d{2}:\\d{2})$', 1) != ''
    THEN from_utc_timestamp(regexp_replace(startTime, '[+-]\\d{2}:\\d{2}$', ''),
                           regexp_extract(startTime, '([+-]\\d{2}:\\d{2})$', 1))
    ELSE cast(regexp_replace(startTime, '[+-]\\d{2}:\\d{2}$', '') as timestamp)
  END as start_time_utc,

  CASE
    WHEN regexp_extract(endTime, '([+-]\\d{2}:\\d{2})$', 1) != ''
    THEN from_utc_timestamp(regexp_replace(endTime, '[+-]\\d{2}:\\d{2}$', ''),
                           regexp_extract(endTime, '([+-]\\d{2}:\\d{2})$', 1))
    ELSE cast(regexp_replace(endTime, '[+-]\\d{2}:\\d{2}$', '') as timestamp)
  END as end_time_utc,

  -- Other fields
  isDaytime,
  temperature,
  temperatureUnit,
  temperatureTrend,

  -- Extract numeric wind speed
  cast(regexp_extract(windSpeed, '(\\d+)', 1) as int) as wind_speed_mph,
  windDirection as wind_direction,

  -- Flatten nested structures
  probabilityOfPrecipitation.value as precipitation_probability,
  dewpoint.value as dewpoint_temp,
  relativeHumidity.value as relative_humidity,

  icon,
  shortForecast as short_forecast,
  detailedForecast as detailed_forecast,

  -- Add processing timestamp
  current_timestamp() as processed_at,
  audit_update_ts
FROM STREAM(LIVE.v_bronze_forecasts_preprocessed)
WHERE post_code IS NOT NULL""")


# ============================================================================
# TARGET TABLES
# ============================================================================

# Create the streaming table
dlt.create_streaming_table(
    name="{{ catalog }}.{{ silver_schema }}.weather_forecasts",
    comment="Streaming table: weather_forecasts",
    table_properties={"quality": "silver", "delta.enableChangeDataFeed": "true"})


# Define append flow(s)
@dlt.append_flow(
    target="{{ catalog }}.{{ silver_schema }}.weather_forecasts",
    name="f_weather_silver",
    comment="Write cleaned weather data to silver layer with SCD Type 1"
)
def f_weather_silver():
    """Write cleaned weather data to silver layer with SCD Type 1"""
    # Streaming flow
    df = spark.readStream.table("v_weather_data_transformed")

    return df
