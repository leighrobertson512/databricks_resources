# Databricks notebook source
# Generated by LakehousePlumber
# Pipeline: weather_data_pipeline
# FlowGroup: weather_data_processing

from pyspark.sql import DataFrame
import dlt

# Pipeline Configuration
PIPELINE_ID = "weather_data_pipeline"
FLOWGROUP_ID = "weather_data_processing"

# ============================================================================
# SOURCE VIEWS
# ============================================================================

@dlt.view()
def v_bronze_forecasts_raw():
    """Load bronze weather forecast data with CDC"""
    df = spark.read.table("{{ catalog }}.{{ bronze_schema }}.{{ bronze_forecasts_table }}")

    return df


# ============================================================================
# TRANSFORMATION VIEWS
# ============================================================================

@dlt.view(comment="Clean and transform weather data with timezone conversion")
def v_weather_data_silver():
    """Clean and transform weather data with timezone conversion"""
    return spark.sql("""SELECT
  post_code,
  number,
  name,
  -- Extract timezone and convert timestamps
  regexp_extract(startTime, '([+-]\\d{2}:\\d{2})$', 1) as timezone_offset,
  regexp_replace(startTime, '[+-]\\d{2}:\\d{2}$', '') as start_time_clean,
  regexp_replace(endTime, '[+-]\\d{2}:\\d{2}$', '') as end_time_clean,

  -- Convert to UTC timestamps
  CASE
    WHEN regexp_extract(startTime, '([+-]\\d{2}:\\d{2})$', 1) != ''
    THEN from_utc_timestamp(regexp_replace(startTime, '[+-]\\d{2}:\\d{2}$', ''),
                           regexp_extract(startTime, '([+-]\\d{2}:\\d{2})$', 1))
    ELSE cast(regexp_replace(startTime, '[+-]\\d{2}:\\d{2}$', '') as timestamp)
  END as start_time_utc,

  CASE
    WHEN regexp_extract(endTime, '([+-]\\d{2}:\\d{2})$', 1) != ''
    THEN from_utc_timestamp(regexp_replace(endTime, '[+-]\\d{2}:\\d{2}$', ''),
                           regexp_extract(endTime, '([+-]\\d{2}:\\d{2})$', 1))
    ELSE cast(regexp_replace(endTime, '[+-]\\d{2}:\\d{2}$', '') as timestamp)
  END as end_time_utc,

  -- Other fields
  isDaytime,
  temperature,
  temperatureUnit,
  temperatureTrend,

  -- Extract numeric wind speed
  cast(regexp_extract(windSpeed, '(\\d+)', 1) as int) as wind_speed_mph,
  windDirection as wind_direction,

  -- Flatten nested structures
  probabilityOfPrecipitation.value as precipitation_probability,
  dewpoint.value as dewpoint_temp,
  relativeHumidity.value as relative_humidity,

  icon,
  shortForecast as short_forecast,
  detailedForecast as detailed_forecast,

  -- Add processing timestamp
  current_timestamp() as processed_at,
  audit_update_ts
FROM STREAM(LIVE.v_bronze_forecasts_raw)
WHERE post_code IS NOT NULL""")

@dlt.view(comment="Aggregate daily weather metrics with one-day lag")
def v_daily_weather_aggregates():
    """Aggregate daily weather metrics with one-day lag"""
    return spark.sql("""SELECT
  post_code,
  date(start_time_utc) as forecast_date,

  -- Temperature aggregations
  min(temperature) as min_temperature,
  max(temperature) as max_temperature,
  avg(temperature) as avg_temperature,

  -- Humidity and precipitation aggregations
  avg(relative_humidity) as avg_humidity,
  max(relative_humidity) as max_humidity,
  min(relative_humidity) as min_humidity,
  avg(precipitation_probability) as avg_precipitation_probability,
  max(precipitation_probability) as max_precipitation_probability,

  -- Wind aggregations
  avg(wind_speed_mph) as avg_wind_speed,
  max(wind_speed_mph) as max_wind_speed,

  -- Dewpoint aggregations
  avg(dewpoint_temp) as avg_dewpoint,
  min(dewpoint_temp) as min_dewpoint,
  max(dewpoint_temp) as max_dewpoint,

  -- Weather condition counts
  count(*) as total_forecasts,
  count(case when short_forecast = 'Sunny' then 1 end) as sunny_forecasts,
  count(case when short_forecast = 'Rain' then 1 end) as rain_forecasts,
  count(case when short_forecast = 'Snow' then 1 end) as snow_forecasts,
  count(case when short_forecast like '%Cloud%' then 1 end) as cloudy_forecasts,

  -- Extreme conditions
  count(case when temperature > 90 then 1 end) as extreme_heat_count,
  count(case when temperature < 32 then 1 end) as freezing_count,
  count(case when precipitation_probability > 80 then 1 end) as high_precip_count,

  -- Metadata
  current_timestamp() as aggregated_at,
  max(processed_at) as latest_source_update

FROM STREAM(LIVE.v_weather_data_silver)
WHERE date(start_time_utc) = current_date() - INTERVAL 1 DAY  -- One day lag
  AND post_code IS NOT NULL
GROUP BY post_code, date(start_time_utc)""")

@dlt.view(comment="Regional weather summary for analytics dashboards")
def v_weather_regional_summary():
    """Regional weather summary for analytics dashboards"""
    return spark.sql("""SELECT
  forecast_date,

  -- Regional aggregations
  count(distinct post_code) as postal_codes_reported,
  avg(min_temperature) as region_min_temp,
  avg(max_temperature) as region_max_temp,
  avg(avg_temperature) as region_avg_temp,

  -- Weather pattern analysis
  avg(avg_precipitation_probability) as region_avg_precip_prob,
  avg(avg_humidity) as region_avg_humidity,
  avg(avg_wind_speed) as region_avg_wind_speed,

  -- Extreme weather summary
  sum(extreme_heat_count) as total_extreme_heat_forecasts,
  sum(freezing_count) as total_freezing_forecasts,
  sum(high_precip_count) as total_high_precip_forecasts,

  -- Weather condition percentages
  round(sum(sunny_forecasts) * 100.0 / sum(total_forecasts), 2) as sunny_percentage,
  round(sum(rain_forecasts) * 100.0 / sum(total_forecasts), 2) as rain_percentage,
  round(sum(snow_forecasts) * 100.0 / sum(total_forecasts), 2) as snow_percentage,
  round(sum(cloudy_forecasts) * 100.0 / sum(total_forecasts), 2) as cloudy_percentage,

  current_timestamp() as summary_created_at

FROM STREAM(LIVE.v_daily_weather_aggregates)
GROUP BY forecast_date""")


# ============================================================================
# TARGET TABLES
# ============================================================================

# Create the streaming table
dlt.create_streaming_table(
    name="{{ catalog }}.{{ silver_schema }}.weather_forecasts",
    comment="Streaming table: weather_forecasts",
    table_properties={"quality": "silver", "delta.enableChangeDataFeed": "true"})


# Define append flow(s)
@dlt.append_flow(
    target="{{ catalog }}.{{ silver_schema }}.weather_forecasts",
    name="f_silver_weather",
    comment="Write cleaned weather data to silver layer with SCD Type 1"
)
def f_silver_weather():
    """Write cleaned weather data to silver layer with SCD Type 1"""
    # Streaming flow
    df = spark.readStream.table("v_weather_data_silver")

    return df


@dlt.table(
    name="{{ catalog }}.{{ gold_schema }}.daily_weather_metrics",
    comment="Materialized view: daily_weather_metrics",
    table_properties={"quality": "gold", "aggregation_level": "daily", "lag_days": "1"},
    refresh_schedule="0 2 * * *")
def daily_weather_metrics():
    """Write to {{ catalog }}.{{ gold_schema }}.daily_weather_metrics from multiple sources"""
    # Materialized views use batch processing
    df = spark.read.table("v_daily_weather_aggregates")

    return df

# Create the streaming table
dlt.create_streaming_table(
    name="{{ catalog }}.{{ gold_schema }}.regional_weather_summary",
    comment="Streaming table: regional_weather_summary",
    table_properties={"quality": "gold", "aggregation_level": "regional", "refresh_pattern": "daily"})


# Define append flow(s)
@dlt.append_flow(
    target="{{ catalog }}.{{ gold_schema }}.regional_weather_summary",
    name="f_regional_summary",
    comment="Regional weather summary table for dashboards"
)
def f_regional_summary():
    """Regional weather summary table for dashboards"""
    # Streaming flow
    df = spark.readStream.table("v_weather_regional_summary")

    return df
