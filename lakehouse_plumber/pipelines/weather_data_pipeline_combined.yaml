pipeline: "weather_data_pipeline"
flowgroup: "weather_data_processing"
description: "Combined weather data pipeline: Bronze → Silver → Gold transformations"

# Use both silver and gold layer presets
presets: ["silver_layer", "gold_layer"]

actions:
  # ============================================================================
  # SILVER LAYER: Bronze → Silver Transformation
  # ============================================================================
  
  # Step 1: Load bronze weather data
  - name: "load_bronze_forecasts"
    type: "load"
    source:
      type: "delta"
      database: "{{ catalog }}.{{ bronze_schema }}"
      table: "{{ bronze_forecasts_table }}"
      cdc_options:
        readChangeFeed: "true"
        maxBytesPerTrigger: "{{ max_bytes_per_trigger }}"
    target: "v_bronze_forecasts_raw"
    description: "Load bronze weather forecast data with CDC"
    
  # Step 2: Transform bronze data to silver
  - name: "transform_to_silver"
    type: "transform"
    transform_type: "sql"
    source: "v_bronze_forecasts_raw"
    target: "v_weather_data_silver"
    sql: |
      SELECT 
        post_code,
        number,
        name,
        -- Extract timezone and convert timestamps
        regexp_extract(startTime, '([+-]\\d{2}:\\d{2})$', 1) as timezone_offset,
        regexp_replace(startTime, '[+-]\\d{2}:\\d{2}$', '') as start_time_clean,
        regexp_replace(endTime, '[+-]\\d{2}:\\d{2}$', '') as end_time_clean,
        
        -- Convert to UTC timestamps
        CASE 
          WHEN regexp_extract(startTime, '([+-]\\d{2}:\\d{2})$', 1) != ''
          THEN from_utc_timestamp(regexp_replace(startTime, '[+-]\\d{2}:\\d{2}$', ''), 
                                 regexp_extract(startTime, '([+-]\\d{2}:\\d{2})$', 1))
          ELSE cast(regexp_replace(startTime, '[+-]\\d{2}:\\d{2}$', '') as timestamp)
        END as start_time_utc,
        
        CASE 
          WHEN regexp_extract(endTime, '([+-]\\d{2}:\\d{2})$', 1) != ''
          THEN from_utc_timestamp(regexp_replace(endTime, '[+-]\\d{2}:\\d{2}$', ''), 
                                 regexp_extract(endTime, '([+-]\\d{2}:\\d{2})$', 1))
          ELSE cast(regexp_replace(endTime, '[+-]\\d{2}:\\d{2}$', '') as timestamp)
        END as end_time_utc,
        
        -- Other fields
        isDaytime,
        temperature,
        temperatureUnit,
        temperatureTrend,
        
        -- Extract numeric wind speed
        cast(regexp_extract(windSpeed, '(\\d+)', 1) as int) as wind_speed_mph,
        windDirection as wind_direction,
        
        -- Flatten nested structures
        probabilityOfPrecipitation.value as precipitation_probability,
        dewpoint.value as dewpoint_temp,
        relativeHumidity.value as relative_humidity,
        
        icon,
        shortForecast as short_forecast,
        detailedForecast as detailed_forecast,
        
        -- Add processing timestamp
        current_timestamp() as processed_at,
        audit_update_ts
      FROM STREAM(LIVE.v_bronze_forecasts_raw)
      WHERE post_code IS NOT NULL
    description: "Clean and transform weather data with timezone conversion"
    
  # Step 3: Write to silver streaming table
  - name: "write_silver_weather"
    type: "write"
    source: "v_weather_data_silver"
    write_target:
      type: "streaming_table"
      database: "{{ catalog }}.{{ silver_schema }}"
      table: "weather_forecasts"
      table_properties:
        quality: "silver"
        delta.enableChangeDataFeed: "true"
    scd_type: 1
    merge_keys: ["post_code", "start_time_clean"]
    sequence_by: "audit_update_ts"
    description: "Write cleaned weather data to silver layer with SCD Type 1"

  # ============================================================================
  # GOLD LAYER: Silver → Gold Aggregations
  # ============================================================================
  
  # Step 4: Create daily aggregations from silver data
  - name: "aggregate_daily_weather"
    type: "transform"
    transform_type: "sql"
    source: "v_weather_data_silver"
    target: "v_daily_weather_aggregates"
    sql: |
      SELECT 
        post_code,
        date(start_time_utc) as forecast_date,
        
        -- Temperature aggregations
        min(temperature) as min_temperature,
        max(temperature) as max_temperature,
        avg(temperature) as avg_temperature,
        
        -- Humidity and precipitation aggregations
        avg(relative_humidity) as avg_humidity,
        max(relative_humidity) as max_humidity,
        min(relative_humidity) as min_humidity,
        avg(precipitation_probability) as avg_precipitation_probability,
        max(precipitation_probability) as max_precipitation_probability,
        
        -- Wind aggregations
        avg(wind_speed_mph) as avg_wind_speed,
        max(wind_speed_mph) as max_wind_speed,
        
        -- Dewpoint aggregations
        avg(dewpoint_temp) as avg_dewpoint,
        min(dewpoint_temp) as min_dewpoint,
        max(dewpoint_temp) as max_dewpoint,
        
        -- Weather condition counts
        count(*) as total_forecasts,
        count(case when short_forecast = 'Sunny' then 1 end) as sunny_forecasts,
        count(case when short_forecast = 'Rain' then 1 end) as rain_forecasts,
        count(case when short_forecast = 'Snow' then 1 end) as snow_forecasts,
        count(case when short_forecast like '%Cloud%' then 1 end) as cloudy_forecasts,
        
        -- Extreme conditions
        count(case when temperature > 90 then 1 end) as extreme_heat_count,
        count(case when temperature < 32 then 1 end) as freezing_count,
        count(case when precipitation_probability > 80 then 1 end) as high_precip_count,
        
        -- Metadata
        current_timestamp() as aggregated_at,
        max(processed_at) as latest_source_update
        
      FROM STREAM(LIVE.v_weather_data_silver)
      WHERE date(start_time_utc) = current_date() - INTERVAL 1 DAY  -- One day lag
        AND post_code IS NOT NULL
      GROUP BY post_code, date(start_time_utc)
    description: "Aggregate daily weather metrics with one-day lag"
    
  # Step 5: Write daily aggregations to gold materialized view
  - name: "write_daily_metrics_mv"
    type: "write"
    source: "v_daily_weather_aggregates"
    write_target:
      type: "materialized_view"
      database: "{{ catalog }}.{{ gold_schema }}"
      table: "daily_weather_metrics"
      refresh_schedule: "0 2 * * *"  # Daily refresh at 2 AM
      table_properties:
        quality: "gold"
        aggregation_level: "daily"
        lag_days: "1"
    cluster_columns: ["forecast_date", "post_code"]
    description: "Materialized view for daily weather aggregations"
    
  # Step 6: Create regional summary from daily aggregations
  - name: "create_regional_summary"
    type: "transform"
    transform_type: "sql"
    source: "v_daily_weather_aggregates"
    target: "v_weather_regional_summary"
    sql: |
      SELECT 
        forecast_date,
        
        -- Regional aggregations
        count(distinct post_code) as postal_codes_reported,
        avg(min_temperature) as region_min_temp,
        avg(max_temperature) as region_max_temp,
        avg(avg_temperature) as region_avg_temp,
        
        -- Weather pattern analysis
        avg(avg_precipitation_probability) as region_avg_precip_prob,
        avg(avg_humidity) as region_avg_humidity,
        avg(avg_wind_speed) as region_avg_wind_speed,
        
        -- Extreme weather summary
        sum(extreme_heat_count) as total_extreme_heat_forecasts,
        sum(freezing_count) as total_freezing_forecasts,
        sum(high_precip_count) as total_high_precip_forecasts,
        
        -- Weather condition percentages
        round(sum(sunny_forecasts) * 100.0 / sum(total_forecasts), 2) as sunny_percentage,
        round(sum(rain_forecasts) * 100.0 / sum(total_forecasts), 2) as rain_percentage,
        round(sum(snow_forecasts) * 100.0 / sum(total_forecasts), 2) as snow_percentage,
        round(sum(cloudy_forecasts) * 100.0 / sum(total_forecasts), 2) as cloudy_percentage,
        
        current_timestamp() as summary_created_at
        
      FROM STREAM(LIVE.v_daily_weather_aggregates)
      GROUP BY forecast_date
    description: "Regional weather summary for analytics dashboards"
    
  # Step 7: Write regional summary to gold streaming table
  - name: "write_regional_summary"
    type: "write"
    source: "v_weather_regional_summary"
    write_target:
      type: "streaming_table"
      database: "{{ catalog }}.{{ gold_schema }}"
      table: "regional_weather_summary"
      table_properties:
        quality: "gold"
        aggregation_level: "regional"
        refresh_pattern: "daily"
    cluster_columns: ["forecast_date"]
    description: "Regional weather summary table for dashboards" 